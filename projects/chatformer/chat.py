"""Methods to chat with pre-trained transformer network."""
import os
import time

import torch
from torch.utils.data import Dataset
import torch.nn.functional as F

from src.data.dataloader import get_dataloader
from src.config.config import Config, init_config
from src.modules.model import CharacterTransformer
from src.utils.tools import load_checkpoint


class Chat:

    def __init__(self, model: torch.nn.Module, dataset: Dataset, config: Config):
        self.model = model
        self.dataset = dataset 
        self.config = config

        self.device = self.config.trainer.device
        self.max_sequence_length = self.config.transformer.max_sequence_length

        # Maximum number of generated tokens.
        self.max_num_tokens = 420
        self.temperature = 1.0
        self.do_sample = True
        self.top_k = 10

    @torch.no_grad()
    def _generate(self, input_text: str) -> str:
        """Generates text from prompt.
        
        Args:
            input_text: Prompt text.

        Returns:
            Text generated by model.     
        """
        # Encode input characters as integer using lookup table from dataloader.
        data = [dataset.char_to_index[char] for char in input_text]

        # Create input tensor from encoded characters.
        x = torch.tensor(data=data, dtype=torch.long)[None,...].to(self.device)

        # Generate some tokens 
        for _ in range(self.max_num_tokens):

            # Make sure that the sequence length is smaller than max sequence length.
            sequence = x if x.size(-1) <= self.max_sequence_length else x[:, -self.max_sequence_length:]

            # Feed sequence into model.
            logits = self.model(sequence)

            # Extract probabilities for last token.
            logits = logits[:, -1, :]

            # Convert logits to probabilities.
            probs = F.softmax(input=logits, dim=-1)

            # Take the most likely next token.
            _, index_next_token = torch.topk(probs, k=1, dim=-1)
            # index_next_token = torch.multinomial(probs, num_samples=1)

            # Add index of most likely token to running sequence.
            x = torch.cat((x, index_next_token), dim=-1) 

        # Remove prompt from sequence:
        x = x[:, len(input_text):]

        output_text = "".join([dataset.index_to_char[int(index)] for index in x[0]])

        return output_text

    def run(self):
        """Runs chat."""
        is_running = True

        print("\nPlease enter a prompt.\n")

        while is_running:
            time.sleep(0.01)

            input_text = input()

            if input_text == "exit":
                is_running = False
            elif input_text == "":
                continue
            # input_text = "What is your purpose?"

            # TODO: Check if input text is not too long (max sequence length).
            # TODO: Check if input characters are valid.
            # TODO: Remove trailing whitespace from input.

            # Feed text to model
            if is_running:
                output_text = self._generate(input_text)
                print(f"\n{output_text}\n")

        print("Bye!")


if __name__ == "__main__":
        
    cwd = os.getcwd()
    print(cwd)

    # Get configuration file
    config = init_config(file_path="config.yml")

    # Get dataloader and dataset.
    # ... Load dataloader to initialize config.
    # ... Get dataset with encoder-decoder methods from dataloader.
    dataloader, _ = get_dataloader(config=config)
    dataset = dataloader.dataset

    # Get the model
    model = CharacterTransformer(config=config)
    # model = CharFormer(config=config)

    ckpt_dir = config.dirs.weights
    model_name = "lexicap" 
    load_checkpoint(model=model, ckpt_dir=ckpt_dir, model_name=model_name)
    config.trainer.device = torch.device("cpu")
    model.to(config.trainer.device)
    model.eval()

    chat = Chat(model=model, dataset=dataset, config=config)
    chat.run()
