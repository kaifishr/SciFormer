"""Methods to chat with pre-trained transformer network."""
import os

import torch
from torch.utils.data import Dataset
import torch.nn.functional as F

from src.data.dataloader import get_dataloader
from src.config.config import Config, init_config
from src.modules.model import CharacterTransformer
from src.utils.tools import load_checkpoint


class Chat:
    """Chat class.

    Uses a autoregressive model to generate text provided a prompt.

    Attributes:
        model: An autoregressive model.
        dataset: Dataset model has been trained on.
        config: Configuration.
        valid_characters: Legal characters.

    """

    def __init__(self, model: torch.nn.Module, dataset: Dataset, config: Config):
        """Initializes chat class."""
        self.model = model
        self.dataset = dataset
        self.config = config

        self.valid_characters = list(self.dataset.char_to_index)

        self.device = self.config.trainer.device
        self.input_sequence_length = self.config.transformer.max_sequence_length

        # Maximum number of generated tokens.
        self.max_num_tokens = 500
        self.temperature = 0.8
        self.do_sample = True 
        self.top_k = 10

    @torch.no_grad()
    def _generate(self, prompt: str) -> str:
        """Generates text from prompt.

        Args:
            input_text: Prompt text.

        Returns:
            Text generated by model.
        """
        # Encode input characters as integer using lookup table from dataloader.
        data = [self.dataset.char_to_index[char] for char in prompt]

        # Create input tensor from encoded characters.
        x = torch.tensor(data=data, dtype=torch.long)[None, ...].to(self.device)

        # Generate some tokens
        for _ in range(self.max_num_tokens):

            # Make sure that the sequence length is smaller than max sequence length.
            sequence = (
                x
                if x.size(-1) <= self.input_sequence_length
                else x[:, -self.input_sequence_length :]
            )

            # Feed sequence into model.
            logits = self.model(sequence)

            # High temperature: make model more creative (text generation).
            # Low temperature: make model more confident (knowledge retrieval).
            # Take first prediction as it is probably associated with the
            # highest confidence.
            logits = logits[:, -1, :] / self.temperature

            # Convert logits to probabilities.
            probabilities = F.softmax(input=logits, dim=-1)

            if self.do_sample:
                index_next_token = torch.multinomial(probabilities, num_samples=1)
            else:
                # Take the most likely next token.
                _, index_next_token = torch.topk(probabilities, k=1, dim=-1)

            # Add index of most likely token to running sequence.
            x = torch.cat((x, index_next_token), dim=-1)

        # Remove prompt from sequence:
        x = x[:, len(prompt) :]

        output = "".join([self.dataset.index_to_char[int(index)] for index in x[0]])

        return output

    def _is_valid_prompt(self, prompt: str) -> bool:
        """Checks if input prompt contains any illegal characters."""
        for character in prompt:
            if character not in self.valid_characters:
                print(f"\nCharacter '{character}' was not part of the training data.")
                return False
        return True

    def _add_padding(self, prompt: str, char: str = " ") -> str:
        """Pads input prompt to have correct size."""
        return prompt.rjust(self.input_sequence_length, " ")

    def test(self):
        """Tests model with some simple prompts."""
        prompts = [
            "Why is there something rather than nothing?",
        ]

        for prompt in prompts:
            print(f"\n{prompt}\n")
            if self._is_valid_prompt(prompt=prompt):
                prompt = self._add_padding(prompt=prompt)
                output = self._generate(prompt=prompt)
                print(f"\n{output}\n")

    def run(self):
        """Runs chat."""
        is_running = True

        while is_running:
            print("\nPlease enter a prompt.\n")

            prompt = input()

            if prompt == "exit":
                is_running = False
            elif prompt == "":
                continue

            # Feed text to model
            if is_running and self._is_valid_prompt(prompt=prompt):
                prompt = self._add_padding(prompt=prompt)
                output = self._generate(prompt=prompt)
                print(f"\n{output}\n")

        print("Bye!")


if __name__ == "__main__":

    cwd = os.getcwd()
    print(cwd)

    # Get configuration file
    config_path = "config.yml"
    config = init_config(file_path=config_path)

    # Load dataloader (also initializes config).
    dataloader, _ = get_dataloader(config=config)

    # Get dataset with encoder-decoder methods.
    dataset = dataloader.dataset

    # Get the model
    model = CharacterTransformer(config=config)

    ckpt_dir = config.dirs.weights
    model_name = config.load_model.model_name
    load_checkpoint(model=model, ckpt_dir=ckpt_dir, model_name=model_name)
    # config.trainer.device = torch.device("cpu")
    model.to(config.trainer.device)
    model.eval()

    chat = Chat(model=model, dataset=dataset, config=config)
    chat.test()
    chat.run()
